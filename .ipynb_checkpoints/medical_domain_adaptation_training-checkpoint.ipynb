{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "configured-vintage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import jiwer\n",
    "from pathlib import Path\n",
    "from random import sample\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "import io\n",
    "import codecs\n",
    "from torchtext import data\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from torchtext.datasets import TranslationDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ordered-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-smooth",
   "metadata": {},
   "source": [
    "### Read ASR Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "quick-rings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read prepared targets data that were used for TTS\n",
    "df_mtsamples = pd.read_csv('normalized_mtsamples.csv',index_col=False)\n",
    "df_mtsamples = df_mtsamples.rename(columns = {'sentence': 'tts_sentence'}, inplace = False)\n",
    "\n",
    "texts_directory=\"/SSD-2T/medical_domain_adaptation_dataset/texts/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "casual-inquiry",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94128/94128 [00:29<00:00, 3159.09it/s]\n"
     ]
    }
   ],
   "source": [
    "def fill_df(df, texts_directory):\n",
    "    \"\"\"Fill dataframe with ASR predictions.\n",
    "       inputs: df - dataframe with target sentences, \n",
    "               texts_directory - path to the folder with ASR outputs\n",
    "    \"\"\"\n",
    "    \n",
    "    text_paths = glob.glob(texts_directory+\"*.txt\")\n",
    "    text_ids = [int(os.path.basename(path).split('.')[0]) for path in text_paths]\n",
    "    df['predicted'] = np.nan\n",
    "    \n",
    "    for i in tqdm(range(len(text_ids))):\n",
    "        text_id = text_ids[i]\n",
    "        file_name = text_paths[i]\n",
    "        file = open(file_name, \"r\")\n",
    "        text = file.readlines()[0]\n",
    "        \n",
    "        text = text.replace('  ',' ')\n",
    "        text=text.strip()\n",
    "        \n",
    "        df['predicted'][text_id]=text\n",
    "            \n",
    "fill_df(df_mtsamples, texts_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "individual-civilization",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94128/94128 [00:00<00:00, 204671.47it/s]\n"
     ]
    }
   ],
   "source": [
    "def fill_targets(df):\n",
    "    \"\"\"Fill dataframe with targets for Transformer.\n",
    "       inputs: df - dataframe with target sentences that were used for TTS\n",
    "    \"\"\"\n",
    "    targets=[]\n",
    "    \n",
    "    for i in tqdm(range(len(df))):\n",
    "        text = df['tts_sentence'][i]\n",
    "        \n",
    "        text = text.replace('.','')\n",
    "        text = text.replace(',','')\n",
    "        text = text.replace(':','')\n",
    "        text = text.replace(';','')\n",
    "        text = text.replace('!','')\n",
    "        text = text.replace('?','')\n",
    "        text = text.replace('(','')\n",
    "        text = text.replace(')','')\n",
    "        text = text.replace('-','')\n",
    "        text = text.replace('_','')\n",
    "        text = text.replace('*','')\n",
    "        text = text.replace('\"','')\n",
    "        text = text.replace('  ',' ')\n",
    "        text = text.strip()\n",
    "        \n",
    "        targets.append(text)\n",
    "        \n",
    "    df['targets']=targets\n",
    "    \n",
    "fill_targets(df_mtsamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accredited-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe\n",
    "df_mtsamples.to_csv('data/medical_domain_adaptation_dataset.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-cameroon",
   "metadata": {},
   "source": [
    "### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rural-seating",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94128/94128 [00:02<00:00, 34441.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean WER: 0.19598329766842254\n",
      "std WER: 0.2544473036273024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_wers(df):\n",
    "    \"\"\"Calculate WER for each target sentence and ASR output.\n",
    "       inputs: df - dataframe with target sentences and ASR outputs.\n",
    "       returns: wers- list of wers.\n",
    "    \"\"\"\n",
    "    wers=[]\n",
    "    for i in tqdm(range(len(df))):\n",
    "        ground_truth=df['targets'][i]\n",
    "        hypothesis = df['predicted'][i]\n",
    "        wer = jiwer.wer(ground_truth, hypothesis)\n",
    "        wers.append(wer)        \n",
    "    return wers\n",
    "\n",
    "wers=get_wers(df_mtsamples)\n",
    "print('Mean WER: ' + str(np.mean(wers)))\n",
    "print('std WER: ' + str(np.std(wers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "specialized-madness",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 94128/94128 [00:00<00:00, 121124.36it/s]\n"
     ]
    }
   ],
   "source": [
    "def apply_transformations(targets, predictions):\n",
    "    \"\"\"Apply normalizations-transformations to sentences and return list of words.\n",
    "       inputs: targets - list of ground truth sentences,\n",
    "               predictions - list of predicted sentences.\n",
    "       returns: references_corpus - list of words for ground truth sentences, \n",
    "                candidate_corpus - list of words for predicted sentences.\n",
    "    \"\"\"\n",
    "    \n",
    "    transformation = jiwer.Compose([\n",
    "                                jiwer.ToLowerCase(),\n",
    "                                jiwer.RemoveMultipleSpaces(),\n",
    "                                jiwer.Strip(),\n",
    "                                jiwer.RemoveEmptyStrings(),\n",
    "                                jiwer.SentencesToListOfWords(word_delimiter=\" \")\n",
    "                                ]) \n",
    "    \n",
    "    references_corpus = []\n",
    "    candidate_corpus = []\n",
    "    \n",
    "    for i in tqdm(range(len(targets))):\n",
    "        trainsformed_target = transformation(targets[i])\n",
    "        trainsformed_prediction = transformation(predictions[i])\n",
    "        references_corpus.append([trainsformed_target])\n",
    "        candidate_corpus.append(trainsformed_prediction)\n",
    "        \n",
    "    return references_corpus, candidate_corpus\n",
    "        \n",
    "\n",
    "references_corpus, candidate_corpus = apply_transformations(df_mtsamples['targets'].tolist(), \n",
    "                                                            df_mtsamples['predicted'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "polish-avatar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.710877001285553\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "# calculate BLEU score\n",
    "bleu_score = bleu_score(candidate_corpus, references_corpus)\n",
    "print('BLEU Score: '+str(bleu_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-guatemala",
   "metadata": {},
   "source": [
    "### Words statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "double-claim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of words in sentence for references corpus: mean - 11.185757691653919, min - 1, max - 42, std - 5.924197336921742\n",
      "Num of words in sentence for candidate corpus: mean - 11.602137514873364, min - 1, max - 41, std - 5.971627533062848\n"
     ]
    }
   ],
   "source": [
    "# Calculate words statistics for dataset\n",
    "num_words_references_corpus = [len(s[0]) for s in references_corpus]\n",
    "num_words_candidate_corpus = [len(s) for s in candidate_corpus]\n",
    "\n",
    "print('Num of words in sentence for references corpus: mean - '+\n",
    "      str(np.mean(num_words_references_corpus))+\n",
    "      ', min - '+str(np.min(num_words_references_corpus))+\n",
    "      ', max - '+str(np.max(num_words_references_corpus))+\n",
    "      ', std - '+str(np.std(num_words_references_corpus)))\n",
    "\n",
    "print('Num of words in sentence for candidate corpus: mean - '+\n",
    "      str(np.mean(num_words_candidate_corpus))+\n",
    "      ', min - '+str(np.min(num_words_candidate_corpus))+\n",
    "      ', max - '+str(np.max(num_words_candidate_corpus))+\n",
    "      ', std - '+str(np.std(num_words_candidate_corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-shark",
   "metadata": {},
   "source": [
    "### Make TorchText Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distributed-things",
   "metadata": {},
   "source": [
    "#### save to separate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "difficult-action",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create directory for dataset\n",
    "Path('data preparation/data/MDA94k').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "horizontal-financing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split randomly on training and validation data\n",
    "validation_length = 3128\n",
    "train_length = len(df_mtsamples)-validation_length\n",
    "\n",
    "indexes = sample(range(len(df_mtsamples)), len(df_mtsamples))\n",
    "train_indexes = indexes[:train_length]\n",
    "validation_indexes = indexes[train_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "norwegian-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_references = [df_mtsamples['targets'][i] for i in train_indexes]\n",
    "validation_references = [df_mtsamples['targets'][i] for i in validation_indexes]\n",
    "\n",
    "train_candidates = [df_mtsamples['predicted'][i] for i in train_indexes]\n",
    "validation_candidates = [df_mtsamples['predicted'][i] for i in validation_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "perceived-cologne",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2txt(data, filepath):\n",
    "    \"\"\"Save list of strings to txt file.\n",
    "       inputs: data - list of strings.\n",
    "       returns: filepath - path with filename of txt file.\n",
    "    \"\"\"\n",
    "    file = open(filepath, \"a+\")\n",
    "    file.writelines('\\n'.join(data))\n",
    "    file.close()\n",
    "\n",
    "# save files\n",
    "list2txt(train_references, filepath='data preparation/data/MDA94k/train.references')  \n",
    "list2txt(validation_references, filepath='data preparation/data/MDA94k/val.references')\n",
    "list2txt(train_candidates, filepath='data preparation/data/MDA94k/train.candidates')  \n",
    "list2txt(validation_candidates, filepath='data preparation/data/MDA94k/val.candidates')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-boards",
   "metadata": {},
   "source": [
    "#### load to TranslationDataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "joined-administration",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n",
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "casual-letters",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/developer/tf1_env/NeMo_env/lib/python3.6/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# load spacy tokenizers\n",
    "spacy_en = en_core_web_sm.load()\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "SRC = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            batch_first = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            batch_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "exotic-orchestra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making TranslationDataset class for our dataset in order to make data in torchtext format\n",
    "class MDA94k(TranslationDataset):\n",
    "    \"\"\"Class for our small custom dataset for Medical Domain Adaptation\"\"\"\n",
    "    \n",
    "    name = 'data preparation/data/MDA94k'\n",
    "    dirname = ''\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, exts, fields, root='',\n",
    "               train='train', validation='val', test=None, **kwargs):\n",
    "        \"\"\"Create dataset objects for splits of the MDA94k dataset.\n",
    "\n",
    "        Arguments:\n",
    "            exts: A tuple containing the extension to path for each language.\n",
    "            fields: A tuple containing the fields that will be used for data\n",
    "                in each language.\n",
    "            root: Root dataset storage directory. Default is '.data'.\n",
    "            train: The prefix of the train data. Default: 'train'.\n",
    "            validation: The prefix of the validation data. Default: 'val'.\n",
    "            test: The prefix of the test data. Default: 'test'.\n",
    "            Remaining keyword arguments: Passed to the splits method of\n",
    "                Dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: This is a _HORRIBLE_ patch related to #208\n",
    "        # 'path' can be passed as a kwarg to the translation dataset constructor\n",
    "        # or has to be set (so the download wouldn't be duplicated). A good idea\n",
    "        # seems to rename the existence check variable from path to something else\n",
    "        if 'path' not in kwargs:\n",
    "            expected_folder = os.path.join(root, cls.name)\n",
    "            path = expected_folder if os.path.exists(expected_folder) else None\n",
    "        else:\n",
    "            path = kwargs['path']\n",
    "            del kwargs['path']\n",
    "\n",
    "        return super(MDA94k, cls).splits(\n",
    "            exts, fields, path, root, train, validation, test, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "parental-passenger",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/developer/tf1_env/NeMo_env/lib/python3.6/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# load our data as torchtext Example\n",
    "train_data, valid_data = MDA94k.splits(exts = ('.candidates', '.references'), \n",
    "                                       fields = (SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "missing-rendering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': ['computer', 'typography', 'head', 'technique'], 'trg': ['computed', 'tomography', 'head', 'technique']}\n"
     ]
    }
   ],
   "source": [
    "# print example\n",
    "print(vars(train_data[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ordinary-testing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': ['as', 'well', 'as', 'right', 'index', 'finger', 'soreness', 'at', 'the', 'peak', 'it', \"'s\", 'territory', 'pressure', 'joint'], 'trg': ['as', 'well', 'as', 'right', 'index', 'finger', 'soreness', 'at', 'the', 'peak', 'inspiratory', 'pressure', 'joint']}\n"
     ]
    }
   ],
   "source": [
    "# print example\n",
    "print(vars(valid_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "czech-circuit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 91000\n",
      "Number of validation examples: 3128\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aggressive-highland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make vocabularies\n",
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "overall-crack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (ASR output) vocabulary: 14674\n",
      "Unique tokens in target (TTS input) vocabulary: 13697\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in source (ASR output) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (TTS input) vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "broke-mandate",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "artificial-sound",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/developer/tf1_env/NeMo_env/lib/python3.6/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# making data iterators\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator = BucketIterator.splits((train_data, valid_data), \n",
    "                                                        batch_size = BATCH_SIZE,\n",
    "                                                        device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-shepherd",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "provincial-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import Encoder, Decoder, Seq2Seq\n",
    "\n",
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "enc = Encoder(INPUT_DIM, \n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "operating-bookmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "resistant-makeup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 14,787,969 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "naval-permit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "julian-equipment",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.apply(initialize_weights);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-joseph",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "enabling-jesus",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "presidential-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "potential-locator",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg = [batch size * trg len - 1]\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "alone-spice",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "            \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "optical-college",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fossil-edward",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/developer/tf1_env/NeMo_env/lib/python3.6/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 35s\n",
      "\tTrain Loss: 3.865 | Train PPL:  47.693\n",
      "\t Val. Loss: 1.873 |  Val. PPL:   6.510\n",
      "Epoch: 02 | Time: 0m 35s\n",
      "\tTrain Loss: 1.534 | Train PPL:   4.636\n",
      "\t Val. Loss: 0.977 |  Val. PPL:   2.656\n",
      "Epoch: 03 | Time: 0m 34s\n",
      "\tTrain Loss: 0.854 | Train PPL:   2.349\n",
      "\t Val. Loss: 0.709 |  Val. PPL:   2.031\n",
      "Epoch: 04 | Time: 0m 34s\n",
      "\tTrain Loss: 0.568 | Train PPL:   1.765\n",
      "\t Val. Loss: 0.609 |  Val. PPL:   1.839\n",
      "Epoch: 05 | Time: 0m 35s\n",
      "\tTrain Loss: 0.418 | Train PPL:   1.519\n",
      "\t Val. Loss: 0.574 |  Val. PPL:   1.776\n",
      "Epoch: 06 | Time: 0m 34s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.558 |  Val. PPL:   1.747\n",
      "Epoch: 07 | Time: 0m 34s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
      "\t Val. Loss: 0.558 |  Val. PPL:   1.748\n",
      "Epoch: 08 | Time: 0m 33s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.544 |  Val. PPL:   1.723\n",
      "Epoch: 09 | Time: 0m 34s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.564 |  Val. PPL:   1.759\n",
      "Epoch: 10 | Time: 0m 33s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.564 |  Val. PPL:   1.758\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'mda94k-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "hazardous-projection",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/developer/tf1_env/NeMo_env/lib/python3.6/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Valid Loss: 0.544 | Valid PPL:   1.723 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('mda94k-model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "print(f'| Valid Loss: {test_loss:.3f} | Valid PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-software",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
